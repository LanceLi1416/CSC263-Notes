\chapter{Graphs}

\begin{figure}[H]
    \tikzset{ every node/.style={draw, circle, minimum size=2em} }
    \centering
    \begin{tikzpicture}[every edge/.style={draw, -latex}] 
        \node (a) at (0,0) {$a$};
        \node (b) at (2,0) {$b$};
        \node (c) at (0,-2) {$c$};
        \node (d) at (2,-2) {$d$};
        \node (e) at (4,-1) {$e$};

        \path (a) edge (b)
              (a) edge (c)
              (b) edge[bend left] (c)
              (b) edge (d)
              (b) edge (e)
              (c) edge[bend left] (b)
              (c) edge (d)
              (d) edge[loop below] (d)
              (d) edge (e);
    \end{tikzpicture}
    \hfil
    \begin{tikzpicture}
        \node (a) at (0,0) {$a$};
        \node (b) at (2,0) {$b$};
        \node (c) at (0,-2) {$c$};
        \node (d) at (2,-2) {$d$};
        \node (e) at (4,-1) {$e$};

        \path (a) edge (b)
              (a) edge (c)
              (b) edge (c)
              (b) edge (d)
              (b) edge (e)
              (c) edge (d)
              (d) edge (e);
    \end{tikzpicture}
\end{figure}

\section{Graphs}

\subsection{Graphs}

Define a graph $G = \{ V, E \}$

\subsubsection{Representations}

\listu {
    \item Adjacency matrix
    
    \begin{center} \begin{tabular}{c | c c c c c}
            & $a$ & $b$ & $c$ & $d$ & $e$ \\ \hline
        $a$ & 0   & 1   & 1   & 0   & 0   \\
        $b$ & 1   & 0   & 1   & 1   & 1   \\
        $c$ & 1   & 1   & 0   & 1   & 0   \\
        $d$ & 0   & 1   & 1   & 0   & 1   \\
        $e$ & 0   & 1   & 0   & 1   & 0
    \end{tabular} \end{center}

    Complexity: let $n = |V|$ and $m = |E|$
    \listu {
        \item Space: $O(n^2)$
        \item Edge query: $\Theta(1)$ time
    }

    \item Adjacency list
    
    % TODO: add figure

    Complexity: let $n = |V|$ and $m = |E|$
    \listu {
        \item Space: $\Theta(n + m)$
        \item Edge query: $\Theta(n)$ in worst-case time
    }
}

\subsection{Breadth-First Search}

In breadth first search, we start at a source $s \in V$, and explore every vertex reachable from $a$, using only edges. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/BFS.png}
\end{figure}

We assign each vertex $v \in V$ a color, which can be one of the following:
\listu {
    \item White: $v$ has not been discovered
    \item Gray: $v$ has been discovered, but not explored
    \item Black: $v$ has been discovered and explored
}

Define $\pi[v]$ to be the predecessor of $v$ in the breadth-first search tree.

Define $d[v]$ to be the distance from $s$ to $v$ in the breadth-first search tree.

We use a queue to keep track of the vertices that we have discovered, but not yet explored.

\clearpage

\begin{algorithm}[H] \begin{algorithmic}[1]
    \Procedure{BFS}{$G$, $s$}
        \State \cmt{Initialize tracking info for all vertices}
        \For {$v \in G.V$}
            \State colour[$v$] = white
            \State $\pi[v] \gets \text{NIL}$
            \State $d[v] \gets \infty$
        \EndFor

        \State \cmt{Initialize empty queue and source vertex tracking info}
        \State $Q \gets \textsc{Make-queue}()$
        \State $\text{colour}[s] \gets \text{gray}$
        \State $\pi[s] \gets \text{NIL}$
        \State $d[s] \gets 0$
        \State \textsc{Enqueue}($Q$, $s$)

        \State \cmt{Main loop. Loop Invariant: $Q$ contains all (and only) gery vertices}
        \While {$Q \neq \textsc{Empty-queue}$}
            \State $u \gets \textsc{Dequeue}()$
            \For {$v \in G.Adj[u]$}
                \If {$\text{colour}[v] \gets \text{white}$}
                    \State $\text{colour}[v] \gets \text{gray}$
                    \State $\pi[v] \gets u$
                    \State $d[v] \gets d[u] + 1$
                    \State \textsc{Enqueue}($Q$, $v$)
                \EndIf
            \EndFor
            \State $\text{colour}[u] = \text{black}$
        \EndWhile
    \EndProcedure
\end{algorithmic} \end{algorithm}

In breath first search, \listu {
    \item Each vertex is enqueued at most once
    \item Each vertex is dequeued at most once
    \item Each adjacent list is examined at most once
    \item The time complexity is $\Theta(n + m)$
}

\subsubsection*{BFS Finds Shortest Paths}

Define $\delta(s, v)$ to be the length of the shortest path from vertex $s$ to vertex $v$ (i.e. the smallest number of edges in any path from $s$ to $v$). If there is no path from $s$ to $v$, then $\delta(s, v) = \infty$. Note that this definition will change when we consider \bred{weighted} graphs.

\theorem { 
    \label{thm:bfs-shortest-path} 
    Let $G = \{ V, E \}$ be a graph, and let $s \in V$. Then, after \textsc{BFS}($G$, $s$), $\forall v \in V$, $\delta(s, v) = d[v]$
}

To prove this theorem, we will need to prove the following lemmas first.

\lemma {
    \label{lem:bfs-shortest-path-1}
    $\forall (u, v) \in E$, $\delta(s, v) \leq \delta(s, u) + 1$
}

\begin{proof}
    (idea)

    If $\delta(s, u) = \infty$, then the claim holds trivially. 

    If $\delta(s, u) \neq \infty$, then $u$ is reachable from $s$.

    Thus, $v$ is also reachable from $s$.

    Thus, the shortest path from $s$ to $v$ is no longer than the shortest path from $s$ to $u$, plus the edge $(u, v)$.

    Hence, $\delta(s, v) \leq \delta(s, u) + 1$.
\end{proof}

\lemma {
    \label{lem:bfs-shortest-path-2}
    At ant point during BFS, $\forall v \in V$, $d[v] \geq \delta(s, v)$
}

\begin{proof}
    (idea)

    Use induction on the number of \textsc{Enqueue} operations.

    Immediately after we do the first \textsc{Enqueue} operation, $d[s] = 0$, and $\delta(s, s) = 0$.

    We also have $d[v] = \infty$, and $\delta(s, v) = \infty$ for all $v \in V - \{ s \}$.

    Now, consider some vertex $v$ that is first discovered while visiting a vertex $u$.

    By the IH, we have $d[u] \geq \delta(s, u)$.

    Hence, $d[v] = d[u] + 1 \geq \delta(s, u) + 1$ by Lemma 4.2.1. %TODO: use \ref

    Then $v$ is painted grey and $d[v]$ is not changed for the rest of the algorithm.
\end{proof}

\lemma {
    \label{lem:bfs-shortest-path-3}
    If $Q = \left< v_1, \dots, v_r \right>$, then $d[v_i] \leq d[v_{i+1}]$ for all $i \in \{ 1, \dots, r-1 \}$ and $d[v_r] \leq d[v_1] + 1$
}

\begin{proof}
    (sketch)

    Use induction on the number of \textsc{Dequeue} / \textsc{Enqueue} operations.

    When $Q = \left< s \right>$, the claim holds trivially.

    To prove the inductive step, we need to show that the lemma hold after applying \textsc{Dequeue} / \textsc{Enqueue} to $Q = \left< v_1, \dots, v_3 \right>$.

    \begin{itemize}
        \item Case 1
        
        If we perform a \textsc{Dequeue} operation, then $Q = \left< v_2, \dots, v_3 \right>$ afterwards.

        By the IH, $d[v_r] \leq d[v_1] + 1$ and $d[v_1] \le d[v_2]$.

        Hence, $d[v_r] \leq d[v_2] + 1$.

        All other inequalities are unaffected. 

        \item Case 2 
        
        If we perform a \textsc{Enqueue} operation, then $Q = \left< v_1, \dots, v_{r+1} \right>$ afterwards.

        We discover $v_{r+1}$ while visiting some vertex $u$, so $d[v_{r+1}] = d[u] + 1$.

        Vertex $u$ must have been the previous vertex dequeued from the queue. 

        Hence, either $v_1$ was discovered while visiting $u$, in which case $[v_1] = d[u] + 1$, or $Q$ was equal to $\left< u_2, v_1, \dots \right>$ at some prior point, in which case $d[u] \leq d[v_1]$ by IH.

        Hence, $d[v_{r+1}] = d[u] + 1 \leq d[v_1] + 1$.

        Otherwise, $d[v_r] \leq d[u] + 1 = d[v_{r+1}]$ by the IH. 
    \end{itemize}
\end{proof}

Now, we can prove the theorem.

\begin{proof}
    To derive a contradiction, suppose $d[v] \neq \delta(s, v)$ for some vertex $v \in V$.

    Suppose $v$ is a vertex with minimal $\delta(s, v)$ for which this is satisfied.

    By Lemma 4.1.2, we have $d[v] > \delta(s, v)$.

    % TODO: add a figure

    Because we chose $v$ with minimal $\delta(s, v)$, we have $d[u] = \delta(s, u)$.

    Hence, $d[v] > \delta(s, v) = \delta(s, u) + 1 = d[u] + 1$.

    Consider the colour of $v$ when we first dequeue $u$ from $Q$.

    \begin{itemize}
        \item If $v$ is painted \textbf{white}, then we set $d[v] = d[u] + 1$, which is a contradiction.
        \item If $v$ is painted \textbf{black}, then $v$ was in the queue before $u$. By Lemma 4.1.3, we have $d[v] \le d[u]$, which is a contradiction.
        \item If $v$ is painted \textbf{grey}, then $v$ was discovered while visiting some vertex $w$ that was dequeued earlier than $u$. Hence, $d [v] = d[w] + 1$, and by Lemma 4.1.3 we have $d[w] \le d[u]$. So $d[v] \le d[u] + 1$, which is a contradiction.
    \end{itemize}
\end{proof}

\subsubsection{Time complexity of BFS}

\listu {
    \item Initialization (painting vertices white, setting entries of $d$ to $\infty$ and entries of $\pi$ to NIL) takes $\Theta(|V|)$ time. 
    \item After initialization, we never paint a vertex white.
    \item Thus, each vertex is enqueued/dequeued at most once. 
    \item Hence, we spend $\mathcal{O}(|V|)$ time doing queue operations. 
    \item Every time we dequeue a vertex, we scan its out-neighbourhood to discover its neighbours:
    \listu {
        \item With an \textbf{adjacency list}, we consider each edge at most once (or at most twice in an undirected graph), so in total we need at most $\mathcal{O}(|E|)$ time to consider all of the edges.
        \item With an adjacency matrix, we scan each row of the matrix at most once, so in total we need at most $\mathcal{O}(|V|^2)$ time to consider all of the edges.
    }
}

\begin{minipage}[t]{0.45\linewidth} \begin{center}
    Using adjacency list: $\mathcal{O}(|V| + |E|)$
\end{center} \end{minipage}
\begin{minipage}[t]{0.45\linewidth} \begin{center}
    Using adjacency matrix: $\mathcal{O}(|V|^2)$
\end{center} \end{minipage}

\subsection{Depth-First Search}

\listu {
    \item In DFS, we walk through the graph as far as possible until we hit a dead end -- when this happens, we backtrack to an undiscovered vertex. 
    \item Similar to BFS, we paint vertices as we go: 
    \listu {
        \item Painted white: undiscovered
        \item Painted grey: discovered but not yet visited 
        \item Painted black: visited
    }
    \item Instead of storing distances/depths, we store timestamps: 
    \item \listu {
        \item $\textit{disc}[v] = \null$time at which $v$ is first discovered
        \item $\textit{vis}[v] = \null$time at which we finish visiting $v$
    }
}

\listu {
    \item One approach is to simply replace the \itblue{queue} from the BFS algorithm with a \itblue{stack}. This gives us an \bred{iterative} DFS algorithm. 
    \item However, it is more natural to write DFS as a recursive algorithm.
}

\begin{minipage}[t]{0.45\linewidth} \begin{algorithm}[H] \begin{algorithmic}[1]
    \Procedure{DFS}{G}
        \State \cmt{Initialization}
        \For {each $v \in G.V$}
            \State $d[v] \gets f[v] \gets \infty$
            \State $\pi[v] \gets \text{NIL}$
        \EndFor
        \State $\text{time} \gets 0$  \cmt{global}
        \State \cmt{Main loop}
        \For {each $v \in G.V$}
            \If {$d[v] = \infty$} 
                \State \cmt{colour[$v$] = white}
                \State \Call{DFS-Visit}{$G$, $v$}
            \EndIf
        \EndFor
    \EndProcedure
\end{algorithmic} \end{algorithm} \end{minipage}
\hfill
\begin{minipage}[t]{0.45\linewidth} \begin{algorithm}[H] \begin{algorithmic}[1]
    \Procedure{DFS-Visit}{$G$, $v$}
        \State \cmt{Discovered (colour[v] = grey)}
        \State $d[v] \gets \text{time} \gets \text{time} + 1$
        \State \cmt{Do something with $v$, if desired}
        \State \cmt{Explore $v$'s adjacency list}
        \For {each $u \in G.adj[v]$}
            \If {$d[u] = \infty$} 
                \State \cmt{colour[$u$] = white}
                \State $\pi[u] = v$
                \State \Call{DFS-Visit}{$G$, $u$}
            \EndIf
        \EndFor
        \State \cmt{Finished (colour[$v$] = black)}
        \State $f[v] \gets \text{time} \gets \text{time} + 1$
    \EndProcedure
\end{algorithmic} \end{algorithm} \end{minipage}

\subsubsection{DFS Forests}

We classify each edge $(u, v)$ based on the colour of $v$ when we consider this edge:

\listu {
    \item \term{Tree edges} are the edges $u$, $v \in E$ that form the DFS forest stored by $\pi$ 
    
    The vertex $v$ is painted \bred{white} when $(u, v)$ is considered 

    \item \term{Back edges}\index{DFS Tree!Back Edge} are the edges $(u, v) \in E$ such that $v$ is an ancestor of $u$ in the DFS forest 
    
    The vertex $v$ is painted \bred{grey} when $(u, v)$ is considered

    \item \term{Forward edges}\index{DFS Tree!Forward Edge} are the edges $(u, v) \in E$ such that $v$ is a descendant of $u$ in the DFS forest 
    
    The vertex $v$ is painted \bred{black} when $(u, v)$ is considered

    \item \term{Cross edges}\index{DFS Tree!Cross Edge} are all other edges $(u, v) \in E$ that are not part of the DFS forest (i.e. $v$ is neither an ancestor nor a descendant of $u$)

    The vertex $v$ is painted \bred{black} when $(u, v)$ is considered
}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/DFS-Tree_Edges.png}
\end{figure}

Note that for undirected graphs, there are NO forward edges and NO cross edges.

\subsubsection{Time Complexity}

The run-time of DFS is similar to BFS.

\begin{minipage}[t]{0.45\linewidth} \begin{center}
    Using adjacency list: $\mathcal{O}(|V| + |E|)$
\end{center} \end{minipage}
\begin{minipage}[t]{0.45\linewidth} \begin{center}
    Using adjacency matrix: $\mathcal{O}(|V|^2)$
\end{center} \end{minipage}

\subsubsection{Parenthesis theorem}\index{Parenthesis Theorem}

\theorem {
    After performing \textsc{DFS}$(G = (V, E))$, for any two vertices $u$, $v \in V$, exactly one of the following statements holds:
    \listo {
        \item The intervals $\left[ \textit{disc}[u], \textit{vis}[u] \right]$ and $\left[ \textit{disc}[v], \textit{vis}[v] \right]$ are disjoint, and neither $u$ nor $v$ is a descendant of the other in the DFS forest.
        \item The interval $\left[ \textit{disc}[u] , \textit{vis}[u] \right]$ is contained entirely in the interval $\left[ \textit{disc}[v], \textit{vis}[v] \right]$, and $u$ is a descendant of $v$ in the DFS forest.
        \item The interval $\left[ \textit{disc}[v], \textit{vis}[v] \right]$ is contained entirely in the interval $\left[ \textit{disc}[u], \textit{vis}[u] \right]$, and $v$ is a descendant of $u$ in the DFS forest.
    }
}

\begin{proof}
    (sketch)

    Suppose that $\textit{disc}[u] < \textit{disc}[v]$.

    \listu {
        \item Case 1: $\textit{disc}[v] < \textit{vis}[u]$

        \begin{center} 
            \includegraphics*[width=0.2\linewidth]{images/Parenthesis-Thm-Grey-u.png}

            $v$ is first discovered while $u$ is painted grey.  
        \end{center}

        So $v$ is a descendant of $u$.

        We don't backtrack to $u$ until we have finished visiting $v$.

        Therefore, we paint $v$ black and set $\textit{vis}[v]$ before backtracking to $u$. Hence, $\textit{vis}[v] < \textit{vis}[u]$.

        \item Case 2: $\textit{vis}[u] < \textit{disc}[v]$

        \begin{center} \includegraphics*[width=0.2\linewidth]{images/Parenthesis-Thm-Black-u.png}

            $v$ is first discovered while $u$ is painted black.  
        \end{center}

        So $v$ is not a descendant of $u$.

       Since $\textit{disc}[u] < \textit{dics}[v]$, $u$ is not a descendant of $v$.

       Since $\textit{disc}[u] < \textit{vis}[u]$ and $\textit{disc}[v] < \textit{vis}[v]$, we have $\textit{disc}[u] < \textit{vis}[u] < \textit{disc}[v] < \textit{vis}[v]$.
    }

    When $\textit{disc}[v] < \textit{disc}[u]$, proof is symmetric.
\end{proof}

\subsection{Strongly connected}

Recall that an undirected graph is connected if and only if there is a path of edges between any pair of vertices in $V$. What about directed graphs? A directed graph $G = (V, E)$ is strongly connected if and only if, for all $u$, $v \in V$, there is a path of edges from $u$ to $v$ in $G$. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.2\linewidth]{images/Strongly-Connected.png}
    \qquad
    \includegraphics[width=0.2\linewidth]{images/Not-Strongly-Connected.png}
    \caption*{A strongly connected graph (left) and a non-strongly connected graph (right, where $v$ is not reachable from $u$)}
\end{figure}

\listu {
    \item Run \textsc{DFS} on the \bred{transpose} of $G$ (reverse edge direction of all edges in $G$)
    \item \itblue{Reorder} vertices in decreasing order of finish time (in $G.V$ and within each adjacency list)

    Run \textit{DFS} on $G$ using the new ordering of vertices

    \item Each DFS tree is one strongly connected component
}

\section{Minimum Spanning Trees}

In a \term{weighted graph}\index{Graph!Weighted Graph} $G = (V, E)$, each edge $e \in E$ has a weight $w(e)$

\begin{center} \includegraphics[width=0.5\linewidth]{images/Weighted-Graph.png} \end{center}

A \term{minimum spanning tree}\index{Minimum Spanning Tree} (MST) $T$ of a weighted, undirected, connected graph $G = (v, E)$ is an acyclic subset of $E$ that connects all of the vertices in $V$ and whose total weight $\displaystyle \sum_{\{u,v\} \in T} w(\{ u, v \} )$ is minimized.

\begin{center} \includegraphics[width=0.5\linewidth]{images/Minimum-Spanning-Tree.png} \end{center}

Above, we define an MST as a set of edges. We also often call the graph $M = (V, T)$ an MST of $G$.

The graph $M = (V, T)$ belongs to a special class of graphs called \bred{trees}, which can be defined in many ways.

\subsection{Prim's Algorithm}\index{Prim's Algorithm}

In Prim's Algorithm, we start with an empty tree $T$, add edges one at a time to $T$ until we have an MST. At each step, pick the \bred{smallest} edge that connects some vertex in $T$ to a vertex outside $T$. 

\begin{center} \includegraphics[width=0.5\linewidth]{images/Prims-Algorithm.png} \end{center}

\begin{algorithm}[H] \begin{algorithmic}
    \Procedure{Prim}{$G$, $w: E \to \mathbb{R}$, $r \in G.V$}
        \State \cmt{Initialization}
        \State $T = \varnothing$
        \State $Q = \textsc{Make-Queue}(G.V)$

        \State \cmt{Place all vertices in the queue}
        \For {$v \in G.V$}
            \State $pri[v] = \infty$
            \State $\pi[v] = \text{NIL}$
            \State $Q.\textsc{Insert}(v)$
        \EndFor

        \State \cmt{Now, set $r$ as the root}
        \State $pri[r] = 0$
        \State $Q.\textsc{Decrease-Key}(r)$

        \State \cmt{Main loop}
        \While {not $Q.\textsc{Empty}()$}
            \State \cmt{Connect a vertex with minimum priority (edge weight)}
            \State $u = Q.\textsc{Extract-Min}()$
            \If {$\pi[u] \neq \textit{NIL}$}
                $T = T \cup \{ \pi[u], u \}$
            \EndIf

            \State \cmt{Update priorities of neighbors of $u$}
            \For {$v \in G.Adj[u]$}
                \If {$v \in Q$ and $w(u, v) < pri[v]$}
                    \State $\pi[v] = u$
                    \State $pri[v] = w(u, v)$
                    \State $Q.\textsc{Decrease-Key}(v)$
                \EndIf
            \EndFor
        \EndWhile
        \State \Return $T$
    \EndProcedure
\end{algorithmic} \end{algorithm}

Runtime

\listu {
    \item Initialization: $\Theta(n)$
    
    \item Main loop: $n$ iterations; one \textsc{Extract-Min} each time 
    
    $\Theta(n \lg n)$

    \item Inner loop: examine each edge twice; at most \textsc{Decrease-Key} per edge
    
    $\Theta(m \lg n)$

    \item Overall: $\Theta((m + n) \lg n)$
}

Note that to make \textsc{Decrease-Key} efficient, we need to track \textit{index}[$v$] -- the position of $v$ in the min-heap -- and update the index values during each swap. 

\subsection{Kruskal's algorithm}

In Kruskal's algorithm, we repeatedly pick the \bred{cheapest edge} of the graph $G$ that \bred{does not create a cycle} in the current set of edges.

\begin{center} \includegraphics[width=0.5\linewidth]{images/Kruskals-Algorithm.png} \end{center}

\begin{algorithm}[H] \begin{algorithmic}[1]
    \Procedure{Kruskal}{$G$, $w$}
        \State \cmt{Sort edges: $w(e_1) \le w(e_2) \le \cdots \le w(e_m)$}
        \State $T = \varnothing$
        \For {each $e_i = e_1, e_2, \ldots, e_m$}
            \State let $(u_i, v_i) = e_i$
            \If {$u_i$, $v_i$ are not already connected by an edge in $T$}
                \State $T = T \cup \{ e_i \}$
            \EndIf
        \EndFor
        \State \Return $T$
    \EndProcedure
\end{algorithmic} \end{algorithm}

To determine if $u_i, v_i$ are already connected, we can use BFS / DFS, which takes $\Theta(n)$ time \footnote{BFS / DFS takes $\Theta(m + n)$ time. Since $m \le n - 1$, this step takes $\Theta(n)$ time.}. The overall run-time would be $\Theta(m \cdot n)$. 

To make this algorithm more efficient, we can use disjoint sets.

\begin{algorithm}[H] \begin{algorithmic}[1]
    \Procedure{Kruskal}{$G$, $w$}
        \State \cmt{Sort edges: $w(e_1) \le w(e_2) \le \cdots \le w(e_m)$}
        \State $T = \varnothing$
        \For $v \in G.V$
            \State \textsc{Make-Set}$(v)$
        \EndFor

        \For {each $e_i = e_1, e_2, \ldots, e_m$}
            \State let $(u_i, v_i) = e_i$
            \If {$\textsc{Find-Set}(u_i) \neq \textsc{Find-Set}(v_i)$}
                \State $T = T \cup \{ e_i \}$
                \State \textsc{Union}$(u_i, v_i)$
            \EndIf
        \EndFor

        \State \Return $T$
    \EndProcedure
\end{algorithmic} \end{algorithm}

\section{Disjoint Sets}\index{Disjoint Sets}

\begin{minipage}[t]{0.4\linewidth}
    \textbf{Data}

    \listu {
        \item A collection $\{S, \dots, S \}$ of disjoint, dynamic sets where $S_i \neq \varnothing$

        Each set in this collection contains a single \term{representative}
    }
\end{minipage}
\begin{minipage}[t]{0.5\linewidth}
    \textbf{Operations}

    \listu {
        \item \textsc{Make-Set}$(x)$
        
        Precondition: $x \in U$, $x \notin S_1, \dots, x \notin S_n$
        
        Create a new set $S_x = \{ x \}$, add $S_x$ to the collection

        \item \textsc{Find-Set}$(x)$
        
        Return the representative element for set $S$ s.t. $x \in S$, or NIL if $x$ no such $S$ exists
    }
\end{minipage}

To check if two elements  belong  to the same set, we can simply check if they have the same representative. Note that we cannot ask for all t he elements of a given set, or for the number of elements in a set.

\subsection{Implementation}

\subsubsection{Implementation 1: Circular Linked List, Boolean \textit{rep}}

\begin{center} \begin{tikzpicture}[
        every node/.style={circle, fill=MyBlue, minimum size=2em},
        every path/.style={-latex}
    ]

    \node (a) at (0, 0) {$\color{white}\hat{x}$};
    \node (b) at (1, 0) {};
    \node (c) at (2, 0) {};
    \node (d) at (3, 0) {};

    \path (a) edge (b)
          (b) edge (c)
          (c) edge (d)
          (d) edge[bend right=30] (a);
\end{tikzpicture} \end{center}

We use a \textit{.rep} field to indicate whether a node is a representative. \textit{.rep} is set to \texttt{true} for the representative of each set, and \texttt{false} for all other nodes.

\listu {
    \item \textsc{Make-Set}$(x)$: $\Theta(1)$

    We only need to create a linked list with a single node

    \item \textsc{Find-Set}$(x)$: $\Theta(n)$
    
    We need to traverse all list nodes to find $x$ and then the representative

    \item \textsc{Union}$(x, y)$: $\Theta(n_1 + n_2)$
    
    We need to first identify the two representatives.\footnote{If the representatives are given, \textsc{Union}$(x, y)$ is $\Theta(1)$.}

    \listu {
        \item If $x$ and $y$ share the same representative, do nothing;
        \item Otherwise, merge the two circular liked lists via changing the \textit{next} attribute for two nodes (conveniently, we choose the representatives, but any two nodes would work), then pick a new representative for the merged set.
    }
}

We should consider the total amortized complexity of $m$ operations. 

\listu {
    \item Upper Bound
    
    Given $m$ operations, we know that each list has size $\le m$. Then, each operations takes time $\mathcal{O}(m)$. In total, the amortized complexity is $\mathcal{O}(m^2)$.

    \item Lower Bound

    Consider $\frac{m}{3}$ \textsc{Make-Set} operations, $\frac{m}{3}$ \textsc{Union} operations, and $\frac{m}{3}$ \textsc{Find-Set} operations.

    The \textsc{Make-Set} and \textsc{Union} operations take $\Omega(m)$ time. The \textsc{Find-Set} operations take $\Omega\left( \frac{m^2}{9} \right)$ time.
    
    Thus, the total time is $\Omega(m^2)$.
}

Thus, we can conclude that the amortized complexity of $m$ operations is $\Theta(m^2)$.

\subsubsection{Implementation 2: Circular Linked List, Pointer to \textit{rep}}

This is similar to implementation 1, except that we use a pointer to the representative instead of a boolean \textit{rep} field.

Now, \textsc{Find-Set} takes $\Theta(1)$ time, and \textsc{Union} takes $\Theta(n_1)$ time since the \textit{.rep} field needs to be updates for one of the to-be-merged sets.

\listu {
    \item Upper Bound
    
    This is the same as implementation 1.

    \item Lower Bound
    
    Consider $\frac{m}{2}$ \textsc{Make-Set} operations and $\frac{m}{2}$ \textsc{Union} operations.

    Then \textsc{Union} operations would take $\displaystyle \sum_{i=1}^{\frac{m}{2}-1} i \in \Omega(m^2)$
}

\subsubsection{Implementation 3: Circular Linked List, Pointer to \textit{rep}, with \textit{size}}

In this implementation, we add a \textit{size} field to \textit{rep} to keep track of the number of elements in the set. During \textsc{Union}, we merge the smaller set into the larger set, so there will be less updates to the \textit{rep} field. We then update the \textit{size} field for the representative of the merged set.

\listu {
    \item Upper Bound

    For any sequence of $m$ operations, let $n$ be the number of \textsc{Make-Set} operations. Then, the total time for all \textsc{Union} operations is less than or equal to the total number of updates to the \textit{rep} field, for all nodes.

    For each element $x$, $x.\textit{rep}$ is updated exactly when the set that contains $x$ is smaller (or the same size) than the other set. Thus, this set must at least double in size after the \textsc{Union} operation. Since there are n \textsc{Make-Set} operations, there can be at most $\log n$ updates to the \textit{rep} attribute for every set.

    In total, the amortized complexity is in $\mathcal{O}(n \log n + m) \in \mathcal{m \log n}$.

    \item Lower Bound

    %TODO: lower bound
}

\subsubsection{Implementation 4: Inverted Trees}

Each node has \textit{parent} attribute, and representative is the root of the tree ($\textit{root.parent} = \textit{root}$). These trees do not have to be binary trees.

\listu {
    \item \textsc{Make-Set}$(x)$: $\Theta(1)$
    
    We only need to create a tree with a single node

    \item \textsc{Find-Set}$(x)$: $\Theta(n)$
    
    We need to fallow \textit{parent} pointers to find the representative. The time depends on how deep the chain / tree is.

    \item \textsc{Union}$(x, y)$: $\Theta(n)$
    
    We need to find the two representatives first via \textsc{Find-Set}, change one of their \textit{parent} pointer to point to the other representative (one more level is added to the chain/tree).
}

Overall, the total worst case of $m$  operations is $\Theta(m^2)$, as it is possible for us to generate a single chain. 

\subsubsection{Implementation 5: Inverted Trees, with \textit{size}}

We are using the same \term{union by weight} idea as implementation 3, Overall, the total worst case running time is $\Theta(m \log m)$.

\subsubsection{Implementation 6: Inverted Trees Path Compression}

The problem is not the total size but rather the height, we want to keep things as close to the root as possible.

\textsc{Find-Set}$(x)$ would follow the pointers, as before, but also add constant-time work to edit the nodes it visits on its way to all point to the root directly as their \textit{parent} attribute. Then, we just need to perform \textsc{FIND-SET}$(x)$ on the bottom of every long chain formed to compress.

\subsubsection{Implementation 7: Inverted Trees Path Compression, with \textit{rank}}

This is a ``\term{union-by-rank}'' idea where rank is an upper bound on height. For \textsc{Make-Set} and \textsc{Union}, the rank can be treated as the same as the height. However, due to path compression, the height may get smaller (we don't want to keep track of/compute that).

During \textsc{Union}, make the root with smaller rank a child of the other tree.

\listu {
    \item If $r_2 < r_1$, make tree with rank $r_2$ a child of tree with rank $r_1$. $r_1$ does not change.
    
    \item If $r_1 = r_2$, make tree with rank $r_2$ a child of tree with rank $r_1$, and increment $r_1$ by 1.
}

Overall, the total worst case run-time of $m$ operations if $\Theta(m \log^* m)$, where $\log^* m$ is the iterated logarithm \footnote{Intuitively, the iterated logarithm represents the number of times the logarithm function must be iteratively applied before the result is less than or equal to 1. For example, $\log^*2 = 1$, $\log^*2^2 = \log^*4 = 1$, $\log^*16 = 3$, $\log^* 2^{16} = 1 + \log^*{16} = 4$, $\log^* 2^{65536} = 1 + \log^* 65536 = 1 + \log^* 2^{16} = 5$}. 